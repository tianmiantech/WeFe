# 逻辑回归

**基础知识回顾**

在逻辑回归中，对于给定的数据集，可以根据极大似然估计法估计出模型的参数。在参数估计计算过程中，为了简化运算通常将似然函数极大化转化为其等价的对似然函数最大值问题。一般将逻辑回归模型的损失函数取对数似然函数的负数得到，损失函数来衡量模型预测错误的程度，损失函数最小则对应对数似然函数大。逻辑回归的损失函数为：

<img src="https://render.githubusercontent.com/render/math?math=loss = -\sum_{n=1}^n [y_i log \pi (x_i) %2b (1-y_i)log(1- \pi(x_i))]">

根据逻辑回归的损失函数可以得到该问题的交叉熵损失函数：

<img src="https://render.githubusercontent.com/render/math?math=J(w) = - \frac {1} {n} \sum_{i=1}^n [y_i log \pi (x_i) +(1-y_i)log(1- \pi(x_i))] =  = - \frac {1} {n} \sum_{i=1}^n [y_i log \, g(w^Tx)) +(1-y_i)log(1- g(w^Tx))] ">

对交叉熵损失函数   J(w)  的其中某一项（第 i 行，第 j 列）求导得到损失函数的梯度值:

<img src="https://render.githubusercontent.com/render/math?math=\frac {\partial(J(w))} {\partial(w_j)}  =  \frac {1} {n} \sum_{i=1}^n (g(w^Tx_i) - y_i)x_{ij}  =\frac {1} {n} \sum_{i=1}^n  (\hat y_i -y_i)x_{ij}= \frac {1} {n} ((\hat y -y)x^j) ">

其中 <img src="https://render.githubusercontent.com/render/math?math=x_{ij}">是第 i 行第 j 列数据，<img src="https://render.githubusercontent.com/render/math?math=x^j"> 是第 j 列数据 ， j = 0,1,2,...,n,    i = 1,2,3,...,n  。于是梯度 <img src="https://render.githubusercontent.com/render/math?math=\nabla J(w)"> ：

<img src="https://render.githubusercontent.com/render/math?math=\nabla J(w) = [\frac {\partial(J(w))} {\partial(w_0)},\frac {\partial(J(w))} {\partial(w_1)},...,\frac {\partial(J(w))} {\partial(w_n)} ]^T = \frac {1} {n} [\sum_{i=1}^n  (\hat y_i -y_i), \sum_{i=1}^n   (\hat y_i -y_i)x_{i1},...,\sum_{i=1}^n  (\hat y_i -y_i)x_{in}]^T"> 



## 纵向逻辑回归

**纵向逻辑回归设计原理**

有以上知识，我们可以讨论联邦学习中逻辑回归的实现方法。在纵向建模中需要不同数据方样本的特征组合来进行建模，假设当前有两个参与方需要进行联和建模，则其数据组合形式应如下图所示：



![image-20220505173613130](images/逻辑回归/image-20220505173613130.png)



逻辑回归传入的训练集样本为 <img src="https://render.githubusercontent.com/render/math?math=x_i = (x_{i0},x_{i1},x_{i2},...,x_{in})^T ">  (其中 <img src="https://render.githubusercontent.com/render/math?math=x_{i0} = 1 ">) ，由公式（3）可以看出在求梯度时需要知道  <img src="https://render.githubusercontent.com/render/math?math=\hat y ">  和 <img src="https://render.githubusercontent.com/render/math?math=x^j"> ，其中 <img src="https://render.githubusercontent.com/render/math?math=\hat y ">  为逻辑回归模型的估计值， <img src="https://render.githubusercontent.com/render/math?math=x^j"> 是数据的特征列。因为纵向联邦学习中各方拥有各自完整的特征列，因此在上述梯度计算公式中， <img src="https://render.githubusercontent.com/render/math?math=x^j"> 是参与各方拥有的数据列无需交换得到，但如果双方想要得到 <img src="https://render.githubusercontent.com/render/math?math=\hat y ">，就需要交换信息了。



(1) y的预测值计算

实现原理很简单，总体 <img src="https://render.githubusercontent.com/render/math?math=w^T x_i "> 可以分为两部分的和，一部分依靠 A 方数据计算得到，另一部分依靠 B 方数据计算得到：

<img src="https://render.githubusercontent.com/render/math?math=w^Tx_i  = w_0 %2b w_1x_{i1} %2b w_1x_{i2} %2b ...%2b w_nx_{in}  = w^T_Ax_{A_i} %2b w^T_Bx_{B_i} ">

如果 A 方把本地的 <img src="https://render.githubusercontent.com/render/math?math=w^T_Ax_{A_i} ">  发送给 B方，B方聚合得到了总体 <img src="https://render.githubusercontent.com/render/math?math=w^T x_i ">，代入梯度计算公式便可以求得 <img src="https://render.githubusercontent.com/render/math?math=\hat y_i"> ，求得   <img src="https://render.githubusercontent.com/render/math?math=\hat y_i">  后就可以根据梯度公式计算各自的在本地计算自己方的梯度了。



(2) 梯度计算

整体   <img src="https://render.githubusercontent.com/render/math?math=\frac {\partial(J(w))} {\partial (w_j)} ">   梯度可分为两部分实现：

<img src="https://render.githubusercontent.com/render/math?math=\frac {\partial(J(w))} {\partial(w)} = (\frac {\partial(J(w))} {\partial(w_A)},\frac {\partial(J(w))} {\partial(w_B)})">

 A 方梯度计算计算如下:

<img src="https://render.githubusercontent.com/render/math?math=\frac {\partial(J(w))} {\partial(w_{A_j})}  =  \frac {1} {n} \sum_{i=1}^n  (\hat y_i -y_i)x_{ij}   = \frac {1} {n} ((\hat y -y)x^j) "> ，其中 j 表示 A 方的第 j 个特征

<img src="https://render.githubusercontent.com/render/math?math=\frac {\partial(J(w))} {\partial(w_0)} = \frac {1} {n} \sum_{i=1}^n  (\hat y_i -y_i)x_{i0} = \frac {1} {n} \sum_{i=1}^n  (\hat y_i -y_i)">，常数项



对于 B 方梯度计算同理可知：

<img src="https://render.githubusercontent.com/render/math?math=\frac {\partial(J(w))} {\partial(w_{B_j})}  =  \frac {1} {n} \sum_{i=1}^n  (\hat y_i -y_i)x_{ij} = \frac {1} {n} ((\hat y -y)x^j)  ">，其中 j 表示 B 方的第 j 个特征

<img src="https://render.githubusercontent.com/render/math?math=\frac {\partial(J(w))} {\partial(w_0)} = \frac {1} {n} \sum_{i=1}^n  (\hat y_i -y_i)x_{i0} = \frac {1} {n} \sum_{i=1}^n  (\hat y_i -y_i)  ">，常数项



**纵向逻辑回归实现流程**

根据从以上推理，可以设计出纵向逻辑回归交互过程：

![image-20220505175918525](images/逻辑回归/image-20220505175918525.png)

<center>参考《Parallel Distributed Logistic Regression for Vertical Federated Learning without Third-Party Coordinator》</center>

算法第2步骤，provider 把 本地的 <img src="https://render.githubusercontent.com/render/math?math=\theta^T_Ax_{A_i}  ">   发送给 promoter，promoter 聚合得到了总体 <img src="https://render.githubusercontent.com/render/math?math=\theta^Tx_i   ">  ，代入梯度计算公式可以得到本地的梯度值。对于provider 第3步将对所有的 i ，使用加法同态算法加密 <img src="https://render.githubusercontent.com/render/math?math=\hat y_i -y_i  "> ，后将加密后的   <img src="https://render.githubusercontent.com/render/math?math=[[\hat y_i -y_i]]">  发送给 provider，provider 根据梯度计算公式可以计算得到本地加密的梯度值 <img src="https://render.githubusercontent.com/render/math?math=[[\frac {d(J(\theta))} {d(\theta_{B})} ]] "> 。

此时 provider 需要解密得到梯度，因为其不具有私钥，其对密文加上同态加密后的随机数掩码发送给 promoter，promoter 同态解密后获得的是加上随机掩码的梯度信息，并不能获得 provider 的原始梯度信息，将解密结果返回给provider ，provider 减去随机数掩码，得到本地梯度。双方计算得到梯度之后可以根据最速下降法或牛顿法来新梯度信息，一次纵向迭代交互完成。当迭代次数达到最大或者损失函数小于指定值时达到收敛时，停止迭代。



## 横向逻辑回归



**横向逻辑回归设计原理**

在应用中为了简化运算，将横向逻辑回归将标签 {0,1} 转化为标签为 {-1,1} 求解，此时的损失函数和损失函数计算分别为：

损失函数：<img src="https://render.githubusercontent.com/render/math?math=J(w) =  \frac {1} {n} \sum_{i=1}^n ln(1 %2b e^{-y_i w^Tx_i})">

梯度：<img src="https://render.githubusercontent.com/render/math?math=\frac {d(J(w)} {d(w_j)}  =  \frac {1} {n} \sum_{i=1}^n (\frac{1}{1 %2b e^{-y_i w^Tx_i}}  e^{-y_i w^Tx_i} (-y_ix_{ij}))=\frac {1} {n} \sum_{i=1}^n  (\frac{1}{1 %2b e^{-y_i w^Tx_i}} -1)y_ix_{ij}  ">

求梯度时需要知道<img src="https://render.githubusercontent.com/render/math?math=w">， <img src="https://render.githubusercontent.com/render/math?math=\hat y"> 和 <img src="https://render.githubusercontent.com/render/math?math=x_{ij}"> ，其中 <img src="https://render.githubusercontent.com/render/math?math=w"> 是逻辑回归模型的系数，是需要模型迭代确定的参数信息， <img src="https://render.githubusercontent.com/render/math?math=\hat y"> 为逻辑回归模型的估计值，<img src="https://render.githubusercontent.com/render/math?math=x_{ij}"> 是数据第i行第j列的的特征值。因为横向联邦学习中各方拥有各自的完整数据，因此在公式(2) 中， <img src="https://render.githubusercontent.com/render/math?math=\hat y"> 和参 <img src="https://render.githubusercontent.com/render/math?math=x_{ij}">与各方无需交换得到，参数 <img src="https://render.githubusercontent.com/render/math?math=w">可以通过第三方聚合加权得到。

**安全聚合**

在实现横向联邦学习时，使用了加法安全聚合技术，即在安全前提下实现不同参数方的数据求和。具体实现原理是多个参与方会根据一定规则使其持有的真实数据加/减一个随机数得到数据的掩码值，掩码值在第三方做加法聚合时各参与方的加减随机数可以抵消，即在不泄露各方真实数据的情况下得到多个参与方的数值和，过程如下图所示：

![image-20211104105348987](images/逻辑回归/image-20211104105348987.png)



**横向逻辑回归流程**

在本系统横向逻辑回归交互时，参数 w 和损失函数 loss 聚合时使用了该加法安全聚合技术（ 第3步和第6步），具体流程如下图所示，这里假设在使用安全聚合时 uuid1< uuid2 < uuid3:

![image-20220110153723927](images/逻辑回归/image-20220110153723927.png)
