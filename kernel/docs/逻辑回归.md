# 逻辑回归

**基础知识回顾**

在逻辑回归中，对于给定的数据集，可以根据极大似然估计法估计出模型的参数。在参数估计计算过程中，为了简化运算通常将似然函数极大化转化为其等价的对似然函数最大值问题。一般将逻辑回归模型的损失函数取对数似然函数的负数得到，损失函数来衡量模型预测错误的程度，损失函数最小则对应对数似然函数大。逻辑回归的损失函数为：

<img src="https://render.githubusercontent.com/render/math?math=loss = -\sum_{n=1}^n [y_i log \pi (x_i) %2b (1-y_i)log(1- \pi(x_i))]">

根据逻辑回归的损失函数可以得到该问题的交叉熵损失函数：

<img src="https://render.githubusercontent.com/render/math?math=J(w) = - \frac {1} {n} \sum_{i=1}^n [y_i log \pi (x_i) +(1-y_i)log(1- \pi(x_i))] =  = - \frac {1} {n} \sum_{i=1}^n [y_i log \, g(w^Tx)) +(1-y_i)log(1- g(w^Tx))] ">

对交叉熵损失函数   J(w)  的其中某一项（第 i 行，第 j 列）求导得到损失函数的梯度值:

<img src="https://render.githubusercontent.com/render/math?math=\frac {\partial(J(w))} {\partial(w_j)}  =  \frac {1} {n} \sum_{i=1}^n (g(w^Tx_i) - y_i)x_{ij}  =\frac {1} {n} \sum_{i=1}^n  (\hat y_i -y_i)x_{ij}= \frac {1} {n} ((\hat y -y)x^j) ">

其中 <img src="https://render.githubusercontent.com/render/math?math=x_{ij}">是第 i 行第 j 列数据，<img src="https://render.githubusercontent.com/render/math?math=x^j"> 是第 j 列数据 ， j = 0,1,2,...,n,    i = 1,2,3,...,n  。于是梯度 <img src="https://render.githubusercontent.com/render/math?math=\nabla J(w)"> ：

<img src="https://render.githubusercontent.com/render/math?math=\nabla J(w) = [\frac {\partial(J(w))} {\partial(w_0)},\frac {\partial(J(w))} {\partial(w_1)},...,\frac {\partial(J(w))} {\partial(w_n)} ]^T = \frac {1} {n} [\sum_{i=1}^n  (\hat y_i -y_i), \sum_{i=1}^n   (\hat y_i -y_i)x_{i1},...,\sum_{i=1}^n  (\hat y_i -y_i)x_{in}]^T"> 



## 纵向逻辑回归

**纵向逻辑回归设计原理**

有以上知识，我们可以讨论联邦学习中逻辑回归的实现方法。在纵向建模中需要不同数据方样本的特征组合来进行建模，假设当前有两个参与方需要进行联和建模，则其数据组合形式应如下图所示：

![image-20220110153355588](images/逻辑回归/image-20220110153355588.png)

逻辑回归传入的训练集样本为 <img src="https://render.githubusercontent.com/render/math?math=x_i = (x_{i0},x_{i1},x_{i2},...,x_{in})^T ">  (其中 <img src="https://render.githubusercontent.com/render/math?math=x_{i0} = 1 ">) ，由公式（3）可以看出在求梯度时需要知道  <img src="https://render.githubusercontent.com/render/math?math=\hat y ">  和 <img src="https://render.githubusercontent.com/render/math?math=x^j"> ，其中 <img src="https://render.githubusercontent.com/render/math?math=\hat y ">  为逻辑回归模型的估计值， <img src="https://render.githubusercontent.com/render/math?math=x^j"> 是数据的特征列。因为纵向联邦学习中各方拥有各自完整的特征列，因此在上述梯度计算公式中， <img src="https://render.githubusercontent.com/render/math?math=x^j"> 是参与各方拥有的数据列无需交换得到，但如果双方想要得到 <img src="https://render.githubusercontent.com/render/math?math=\hat y ">，就需要交换信息了。



(1) y的预测值计算

实现原理很简单，总体 <img src="https://render.githubusercontent.com/render/math?math=w^T x_i "> 可以分为两部分的和，一部分依靠 A 方数据计算得到，另一部分依靠 B 方数据计算得到：

<img src="https://render.githubusercontent.com/render/math?math=w^Tx_i  = w_0 %2b w_1x_{i1} %2b w_1x_{i2} %2b ...%2b w_nx_{in}  = w^T_Ax_{A_i} %2b w^T_Bx_{B_i} ">

如果 A 方把本地的 <img src="https://render.githubusercontent.com/render/math?math=w^T_Ax_{A_i} ">  发送给 B方，B方聚合得到了总体 <img src="https://render.githubusercontent.com/render/math?math=w^T x_i ">，代入梯度计算公式便可以求得 <img src="https://render.githubusercontent.com/render/math?math=\hat y_i"> ，求得   <img src="https://render.githubusercontent.com/render/math?math=\hat y_i">  后就可以根据梯度公式计算各自的在本地计算自己方的梯度了。



(2) 梯度计算

整体   <img src="https://render.githubusercontent.com/render/math?math=\frac {\partial(J(w))} {\partial (w_j)} ">   梯度可分为两部分实现：

<img src="https://render.githubusercontent.com/render/math?math=\frac {\partial(J(w))} {\partial(w)} = (\frac {\partial(J(w))} {\partial(w_A)},\frac {\partial(J(w))} {\partial(w_B)})">

 A 方梯度计算计算如下:

<img src="https://render.githubusercontent.com/render/math?math=\frac {\partial(J(w))} {\partial(w_{A_j})}  =  \frac {1} {n} \sum_{i=1}^n (g(w^Tx_i) - y_i)x_{ij}=\frac {1} {n} \sum_{i=1}^n  (\hat y_i -y_i)x_{ij}   = \frac {1} {n} ((\hat y -y)x^j) "> ，其中 j 表示 A 方的第 j 个特征

<img src="https://render.githubusercontent.com/render/math?math=\frac {\partial(J(w))} {\partial(w_0)} = \frac {1} {n} \sum_{i=1}^n  (\hat y_i -y_i)x_{i0} = \frac {1} {n} \sum_{i=1}^n  (\hat y_i -y_i)">，常数项



对于 B 方梯度计算同理可知：

<img src="https://render.githubusercontent.com/render/math?math=\frac {\partial(J(w))} {\partial(w_{B_j})}  =  \frac {1} {n} \sum_{i=1}^n (g(w^Tx_i) - y_i)x_{ij}=\frac {1} {n} \sum_{i=1}^n  (\hat y_i -y_i)x_{ij} = \frac {1} {n} ((\hat y -y)x^j)  ">，其中 j 表示 B 方的第 j 个特征

<img src="https://render.githubusercontent.com/render/math?math=\frac {\partial(J(w))} {\partial(w_0)} = \frac {1} {n} \sum_{i=1}^n  (\hat y_i -y_i)x_{i0} = \frac {1} {n} \sum_{i=1}^n  (\hat y_i -y_i)  ">，常数项



**纵向逻辑回归实现流程**

根据从以上推理，可以设计出纵向逻辑回归交互过程：

![image-20211104104856219](images/逻辑回归/image-20211104104856219.png)

![image-20211104104944974](images/逻辑回归/image-20211104104944974.png)

<center>论文《Parallel Distributed Logistic Regression for Vertical Federated Learning without Third-Party Coordinator》</center>

算法第2步骤，provider 把 本地的 <img src="https://render.githubusercontent.com/render/math?math=\theta^T_Ax_{A_i}  ">   发送给 promoter，promoter 聚合得到了总体 <img src="https://render.githubusercontent.com/render/math?math=\theta^Tx_i   ">  ，代入梯度计算公式可以得到本地的梯度值。对于provider 第3步将对所有的 i ，使用加法同态算法加密 <img src="https://render.githubusercontent.com/render/math?math=\hat y_i -y_i  "> ，后将加密后的   <img src="https://render.githubusercontent.com/render/math?math=[[\hat y_i -y_i]]">  发送给 provider，provider 根据梯度计算公式可以计算得到本地加密的梯度值 <img src="https://render.githubusercontent.com/render/math?math=[[\frac {d(J(\theta))} {d(\theta_{B})} ]] "> 。

此时 provider 需要解密得到梯度，因为其不具有私钥，其对密文加上同态加密后的随机数掩码发送给 promoter，promoter 同态解密后获得的是加上随机掩码的梯度信息，并不能获得 provider 的原始梯度信息，将解密结果返回给provider ，provider 减去随机数掩码，得到本地梯度。双方计算得到梯度之后可以根据最速下降法或牛顿法来新梯度信息，一次纵向迭代交互完成。当迭代次数达到最大或者损失函数小于指定值时达到收敛时，停止迭代。





## 横向逻辑回归

**横向逻辑回归设计原理**

根据基础部分得到的梯度计算方法如下，在求梯度时需要知道  <img src="https://render.githubusercontent.com/render/math?math=\hat y">  和  <img src="https://render.githubusercontent.com/render/math?math=x^j">，其中 <img src="https://render.githubusercontent.com/render/math?math=\hat y"> 为逻辑回归模型的估计值， <img src="https://render.githubusercontent.com/render/math?math=x^j">  是数据的特征列。因为横向联邦学习中各方拥有各自完整的样本信息，因此在公式(1) 中计算梯度中， <img src="https://render.githubusercontent.com/render/math?math=x^j">  和  <img src="https://render.githubusercontent.com/render/math?math=\hat y">参与各方无需交换得到，可根据自己数据集计算，最后计算结果后按照样本数量 i  做加和即可。

<img src="https://render.githubusercontent.com/render/math?math=\frac {d(J(w))} {d(w_j)}  =  \frac {1} {n} \sum_{i=1}^n (g(w^Tx_i) - y_i)x_{ij}  = \frac {1} {n} ((\hat y -y)x^j) ">

本系统中将横向逻辑回归将标签 {0,1} 转化为标签为 {-1,1} ，此时推理可得此时损失函数和损失函数计算分别为：

损失函数：<img src="https://render.githubusercontent.com/render/math?math=J(w) =  \frac {1} {n} \sum_{i=1}^n ln(1+e^{-y_i w^Tx_i})">

梯度：<img src="https://render.githubusercontent.com/render/math?math=\frac {d(J(w)} {d(w_j)}  =  \frac {1} {n} \sum_{i=1}^n (\frac{1}{1+e^{-y_i w^Tx_i}} * e^{-y_i w^Tx_i}*-y_ix_{ij})=\frac {1} {n} \sum_{i=1}^n  (\frac{1}{1+e^{-y_i w^Tx_i}} -1)y_ix_{ij}  ">

根据损失函数和梯度计算公式，双方分别各自本地计算，之后使用安全聚合方法就可以得到总体的损失函数和梯度值，以下以两个参与方举例，多个时情况类似。

**横向逻辑回归流程**

本平台在实现横向联邦学习时，使用了安全加法聚合技术，安全加法聚合即根据一定规则使多个参与方的真实数据加减相同的随机数得到数据的掩码值，掩码值在第三方做加法聚合时各参与方的加减随机数可以抵消，在不泄露各方真实数据的情况下得到多个参与方的数值和。在本系统横向逻辑回归交互时，参数 w 和损失函数loss 聚合时使用了该加法安全聚合技术（ 第3步和第6步），具体流程如下图所示，这里假设在使用安全聚合时 uuid1< uuid2 < uuid3:

![image-20220110153723927](images/逻辑回归/image-20220110153723927.png)

本系统安全聚合的流程介绍如下：

![image-20211104105348987](images/逻辑回归/image-20211104105348987.png)

